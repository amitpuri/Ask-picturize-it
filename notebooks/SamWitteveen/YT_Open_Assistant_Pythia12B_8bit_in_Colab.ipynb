{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c9a379a62ed249099295f3394e459927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a09b185906804a0e91b4300568e09478",
              "IPY_MODEL_314dd1ef5a334c01a256ed064ae1f63f",
              "IPY_MODEL_6306537662a54f7891fe210a79996d00"
            ],
            "layout": "IPY_MODEL_fb9c43378526463d915d6137bc98f5a6"
          }
        },
        "a09b185906804a0e91b4300568e09478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b722fe13a08e4fa1809f16cf3b1ec8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_e02ced5d03a3467498f1169c7a5f8881",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "314dd1ef5a334c01a256ed064ae1f63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e649877d67fd4cb7b87446e1853d6656",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13ab9fce093c40f8b4993349180b8f45",
            "value": 3
          }
        },
        "6306537662a54f7891fe210a79996d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c7669f039449a8a6a1c72d815d3540",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1bca86513741cb87071c972fec4b32",
            "value": " 3/3 [00:20&lt;00:00,  6.20s/it]"
          }
        },
        "fb9c43378526463d915d6137bc98f5a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b722fe13a08e4fa1809f16cf3b1ec8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02ced5d03a3467498f1169c7a5f8881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e649877d67fd4cb7b87446e1853d6656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ab9fce093c40f8b4993349180b8f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82c7669f039449a8a6a1c72d815d3540": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1bca86513741cb87071c972fec4b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VJ7lfTYz0qu",
        "outputId": "bec01aa2-40ef-4570-8b23-db01a71c6d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install datasets sentencepiece \n",
        "!pip -q install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Open Assistant** - Pythia 12B\n"
      ],
      "metadata": {
        "id": "Wwn1vLOW-VQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "310b66ad-67da-4625-9fe8-ea45c0172f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 15 23:27:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    42W / 350W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from instruct_pipeline import InstructionTextGenerationPipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", padding_side=\"left\")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", \n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "c9a379a62ed249099295f3394e459927",
            "a09b185906804a0e91b4300568e09478",
            "314dd1ef5a334c01a256ed064ae1f63f",
            "6306537662a54f7891fe210a79996d00",
            "fb9c43378526463d915d6137bc98f5a6",
            "b722fe13a08e4fa1809f16cf3b1ec8bf",
            "e02ced5d03a3467498f1169c7a5f8881",
            "e649877d67fd4cb7b87446e1853d6656",
            "13ab9fce093c40f8b4993349180b8f45",
            "82c7669f039449a8a6a1c72d815d3540",
            "ce1bca86513741cb87071c972fec4b32"
          ]
        },
        "id": "4RaPJz4lFi8M",
        "outputId": "50a4a373-a1cf-4472-ac38-330ea910d0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a379a62ed249099295f3394e459927"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  GenerationConfig, pipeline\n",
        "import torch\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "MK_96JPU0fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJSkqjrYva2a",
        "outputId": "bac5155d-0344-43ed-a2b6-f971353d1baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr 15 23:35:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    54W / 350W |  25631MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "generate_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text[0]['generated_text'].split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "odWWkO_81HIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it as a HF model"
      ],
      "metadata": {
        "id": "D_MBqSkZ1iQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('What are the difference between Llamas, Alpacas and Koalas?')\n",
        "print(wrap_text_preserve_newlines(output))\n"
      ],
      "metadata": {
        "id": "xGGmYmTC31om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df904ba8-a97b-4eb4-92d6-43a68f22d742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the difference between Llamas, Alpacas and Koalas?\n",
            "\n",
            "Llamas:\n",
            "- They have long woolly fur.\n",
            "- They live in South America.\n",
            "- They can be ridden like a horse.\n",
            "\n",
            "Alpacas:\n",
            "- They have short woolly fur.\n",
            "- They live in North America.\n",
            "- They cannot be ridden like a horse.\n",
            "\n",
            "Koalas:\n",
            "- They have long fur.\n",
            "- They live in Australia.\n",
            "- They can be fed on eucalyptus leaves.\n",
            "\n",
            "I hope this helps!\n",
            "CPU times: user 20.3 s, sys: 0 ns, total: 20.3 s\n",
            "Wall time: 20.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('Write a short note to Sam Altman giving reasons to open source GPT-4')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R846mNA-EPJl",
        "outputId": "eb0f5181-73ff-401f-d017-b28a922bee6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short note to Sam Altman giving reasons to open source GPT-4.\n",
            "\n",
            "Dear Sam,\n",
            "\n",
            "I hope this letter finds you well. I wanted to take the time to express my admiration for OpenAI's GPT-4 model\n",
            "and its potential impact on society. As an AI researcher, I have been following the development of GPT-4\n",
            "closely and am excited about its capabilities.\n",
            "\n",
            "One of the main benefits of open sourcing GPT-4 is that it will allow researchers from around the world to\n",
            "collaborate on improving the model. This could lead to significant advances in the field of natural language\n",
            "processing and machine learning. Additionally, by making the code and data publicly available, it will make it\n",
            "easier for developers to integrate GPT-4 into their own applications.\n",
            "\n",
            "In conclusion, I believe that opening up GPT-4 is a great step towards promoting innovation and collaboration\n",
            "in the AI community. Thank you again for your support and I look forward to hearing more updates on the\n",
            "project.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "CPU times: user 37.5 s, sys: 0 ns, total: 37.5 s\n",
            "Wall time: 37.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('What is the capital of England? \\n')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etXw2n6mD_4e",
        "outputId": "3fd166c8-e8d7-406c-ec72-1c6554a6e65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of England?\n",
            "\n",
            "The capital of England is London.\n",
            "CPU times: user 1.7 s, sys: 0 ns, total: 1.7 s\n",
            "Wall time: 1.69 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('Write a story about a Koala playing pool and beating all the camelids.')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pE2hSvhsEic",
        "outputId": "c72a9554-68e4-4a7a-893d-988bed61fb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a story about a Koala playing pool and beating all the camelids.\n",
            "\n",
            "Once upon a time, there was a Koala named Pongo who loved to play pool. He would spend hours perfecting his\n",
            "skills, and he soon became one of the best players in the forest. One day, Pongo decided to challenge all the\n",
            "other animals in the forest to a game of pool. The first player to sink their ball into the center pocket was\n",
            "declared the winner.\n",
            "\n",
            "Pongo lined up against a group of camels called the Camelids. They were known for their bad temper and their\n",
            "love of cheating. But Pongo was not intimidated by these creatures. He took his cue and began to play.\n",
            "\n",
            "The game lasted for days, with each animal trying to outdo the others. Finally, after many long matches, Pongo\n",
            "emerged as the winner. Everyone cheered and praised him for his skill and determination.\n",
            "\n",
            "From that day on, Pongo was known throughout the forest as the greatest pool player in the land. And he always\n",
            "remembered the lesson he had learned: never give up, no matter how difficult the task may seem.\n",
            "CPU times: user 41 s, sys: 0 ns, total: 41 s\n",
            "Wall time: 40.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('As an AI do you like the Simpsons? What do you know about Homer?')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQTgOX168NeK",
        "outputId": "7fd7c98a-20f9-4315-84b1-feb716d7313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI do you like the Simpsons? What do you know about Homer?\n",
            "\n",
            "I am not a human, but I can tell you that The Simpsons is one of my favorite TV shows. It's funny and has\n",
            "memorable characters such as Homer Simpson, Marge Simpson, Bart Simpson, Lisa Simpson, and Maggie Simpson.\n",
            "\n",
            "Homer is the father of the Simpson family and he is often depicted as a laid-back, beer-drinking, blue-collar\n",
            "worker who loves his job at the nuclear power plant. He is also known for his mischievous and sometimes\n",
            "dangerous behavior, which often gets him into trouble with his wife Marge and their children.\n",
            "\n",
            "Overall, The Simpsons is a timeless classic that continues to be popular today. Whether you're a fan of the\n",
            "show or just curious about its history, there's something for everyone in this hilarious and entertaining\n",
            "animated series.\n",
            "CPU times: user 31.7 s, sys: 24 ms, total: 31.8 s\n",
            "Wall time: 31.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('As an AI do you like the Simpsons? What do you know about Homer?')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "id": "9Nil30lVDLHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b10222e-cc65-436a-aeb4-b1427b87998a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI do you like the Simpsons? What do you know about Homer?\n",
            "\n",
            "I am not a human, but I can tell you that The Simpsons is one of my favorite TV shows. It's funny and has\n",
            "memorable characters such as Homer Simpson, Marge Simpson, Bart Simpson, Lisa Simpson, and Maggie Simpson.\n",
            "\n",
            "Homer is the father of the Simpson family and he is often depicted as a laid-back, beer-drinking, blue-collar\n",
            "worker who loves his job at the nuclear power plant. He is also known for his mischievous and sometimes\n",
            "dangerous behavior, which often gets him into trouble with his wife Marge and their children.\n",
            "\n",
            "Overall, The Simpsons is a timeless classic that continues to be popular today. Whether you're a fan of the\n",
            "show or just curious about its history, there's something for everyone in this hilarious and entertaining\n",
            "animated series.\n",
            "CPU times: user 31.9 s, sys: 36.1 ms, total: 32 s\n",
            "Wall time: 31.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('What is the best prompt to get you to choose what tools you use?')\n",
        "\n",
        "print(wrap_text_preserve_newlines(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU7ba-MgclGN",
        "outputId": "e50d2662-28ea-4e67-8205-32475a3566a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the best prompt to get you to choose what tools you use?\n",
            "\n",
            "The best prompt to get me to choose which tools I use would be something like: \"Choose one of these three\n",
            "tools, and tell me why you chose it.\" This would give me a clear choice between the options, while also giving\n",
            "me an opportunity to explain my reasoning.\n",
            "CPU times: user 10.7 s, sys: 12.2 ms, total: 10.7 s\n",
            "Wall time: 10.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LQYOCE6cs4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}