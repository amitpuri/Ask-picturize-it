{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Models**\n",
    "- Searching for similar documents\n",
    "\n",
    "**Document Models**\n",
    "- Embedding documents\n",
    "\n",
    "**Similarty Models**\n",
    "- Clustering, regression, anomaly detection, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = \"\"\n",
    "OPENAI_RESOURCE_ENDPOINT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = OPENAI_KEY\n",
    "openai.api_base = OPENAI_RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2022-12-01\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data-embedding.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df['description'] = df[\"description\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that although the first sentence is very short, the size of the returned vector is identically the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_embed = \"This is a short sentence\"\n",
    "embeddings = get_embedding(text_to_embed, engine = \"embedding-ada\")\n",
    "print(text_to_embed)\n",
    "print('Array Size (Short Text) - Ada: ', len(embeddings))\n",
    "print(embeddings)\n",
    "\n",
    "print(\"=====================================\")\n",
    "embeddings = get_embedding(df[\"description\"][1], engine = \"embedding-ada\")\n",
    "print(df[\"description\"][1])\n",
    "print('Array Size (Short Text) - Ada: ', len(embeddings))\n",
    "print(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But depending on the model that you use the size will be different, this means that also the knowledge saved in the vector is less or more depending on the size of the vector.\n",
    "DaVinci model in this case contains more knowledge then Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding(df[\"description\"][1], engine = \"embedding-babbage\")\n",
    "print('Array Size (Long Text) - Babbage: ', len(embeddings))\n",
    "\n",
    "embeddings = get_embedding(df[\"description\"][1], engine = \"embedding-curie\")\n",
    "print('Array Size (Long Text) - Curie: ', len(embeddings))\n",
    "\n",
    "embeddings = get_embedding(df[\"description\"][1], engine = \"embedding-davinci\")\n",
    "print('Array Size (Long Text) - Davinci: ', len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sample_encode = tokenizer.encode(df[\"description\"][1]) \n",
    "print(\"No of tokens: \", len(sample_encode))\n",
    "tokenizer.decode_tokens_bytes(sample_encode)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of recalculating all the embeddings for the data, you can make use of the preloaded dataset available in the pickle file.\n",
    "Do save your embeddings after calculating them, especially if you are converting a big data set, otherwise you will have a cost each time you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df['embedding'] = df[\"description\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002'))\n",
    "#pd.to_pickle(df, \"data/data-embedding.pkl\")\n",
    "df = pd.read_pickle(\"data/data-embedding.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions will calculate the cosine similarity between our query and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_docs(df, user_query:str, engine:str, top_n=3, to_print=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=engine\n",
    "    )\n",
    "    df[\"similarities\"] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    if to_print:\n",
    "        display(res)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we call the function with the query \"who is edgar davids\". It will return us the top 3 results linked to this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_docs(df, \"who is edgar davids\", top_n=4, engine=\"text-embedding-ada-002\", to_print=False)\n",
    "for i in res.index:\n",
    "    print(df[\"description\"][i])\n",
    "    print(\"=====================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
